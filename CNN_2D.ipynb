{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 369,
      "metadata": {
        "id": "-hSQNtixyoFQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qvDK_x8Hyy4_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9qAL_eUyzDu",
        "outputId": "82942e96-d4f1-4815-c57e-6b759519011c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 370,
      "metadata": {
        "id": "rckV43vcyoFS"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "\n",
        "def cross_entropy(x):\n",
        "    return -np.log(x)\n",
        "\n",
        "\n",
        "def regularized_cross_entropy(layers, lam, x):\n",
        "    loss = cross_entropy(x)\n",
        "    for layer in layers:\n",
        "        loss += lam * (np.linalg.norm(layer.get_weights()) ** 2)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def leakyReLU(x, alpha=0.001):\n",
        "    return np.where(x < 0, x * alpha, x)\n",
        "\n",
        "\n",
        "def leakyReLU_derivative(x, alpha=0.01):\n",
        "    return np.where(x < 0, alpha, 1)\n",
        "def ReLU(Z):\n",
        "        return np.maximum(Z, 0)\n",
        "def ReLU_deriv(Z):\n",
        "        return Z > 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 371,
      "metadata": {
        "id": "PC-Sn6vwyoFT"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "class ConvLayer:\n",
        "    def __init__(self, activation, padding=\"same\", stride=(1, 1)):\n",
        "\n",
        "        self.activation = activation\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "        self.w=None\n",
        "        self.b=None\n",
        "\n",
        "    def forward_prop(self,A_prev,W_conv,b_conv):\n",
        "\t    #forward prop convolutional 3D image, RGB image - color\n",
        "\n",
        "        # Arg:\n",
        "\t    #    A_prev: contains the output of prev layer (m, h_prev, w_prev, c_prev)\n",
        "\t    #    W_conv: filter for the convolution (kh, kw, c_prev, c_new)\n",
        "\t    #    b_conv: biases (1, 1, 1, c_new)\n",
        "\t    #    padding: string ‘same’, or ‘valid’\n",
        "\t    #    stride: tuple (sh, sw)\n",
        "        # Return: padded convolved images RGB np.array\n",
        "        m, h_prev, w_prev, c_prev = A_prev.shape\n",
        "        self.w=W_conv\n",
        "        self.b=b_conv\n",
        "        k_h, k_w, c_prev, c_new = self.w.shape\n",
        "\n",
        "        s_h, s_w = self.stride\n",
        "        if self.padding == 'valid':\n",
        "            p_h = 0\n",
        "            p_w = 0\n",
        "        if self.padding == 'same':\n",
        "            p_h = np.ceil(((s_h*h_prev) - s_h + k_h - h_prev) / 2)\n",
        "            p_h = int(p_h)\n",
        "            p_w = np.ceil(((s_w*w_prev) - s_w + k_w - w_prev) / 2)\n",
        "            p_w = int(p_w)\n",
        "        A_prev = np.pad(A_prev, [(0, 0), (p_h, p_h), (p_w, p_w), (0, 0)],\n",
        "                        mode='constant', constant_values=0)\n",
        "\n",
        "        out_h = int(((h_prev - k_h + (2*p_h)) / (self.stride[0])) + 1)\n",
        "        out_w = int(((w_prev - k_w + (2*p_w)) / (self.stride[1])) + 1)\n",
        "        output_conv = np.zeros((m, out_h, out_w, c_new))\n",
        "        m_A_prev = np.arange(0, m)\n",
        "\n",
        "        for i in range(out_h):\n",
        "            for j in range(out_w):\n",
        "                for f in range(c_new):\n",
        "                    output_conv[m_A_prev, i, j, f] = self.activation((\n",
        "                        np.sum(np.multiply(\n",
        "                            A_prev[\n",
        "                                m_A_prev,\n",
        "                                i*(self.stride[0]):k_h+(i*(self.stride[0])),\n",
        "                                j*(self.stride[1]):k_w+(j*(self.stride[1]))],\n",
        "                            self.w[:, :, :, f]), axis=(1, 2, 3))) + self.b[0, 0, 0, f])\n",
        "\n",
        "\n",
        "        return output_conv\n",
        "\n",
        "    def backward_prop(self,dZ, A_prev):\n",
        "\t    #back prop convolutional 3D image, RGB image - color\n",
        "\t    # Arg:\n",
        "\t    #    dZ: containing the partial derivatives (m, h_new, w_new, c_new)\n",
        "\t    #    A_prev: contains the output of prev layer (m, h_prev, w_prev, c_prev)\n",
        "\t    #    W: filter for the convolution (kh, kw, c_prev, c_new)\n",
        "\t    #    b: biases (1, 1, 1, c_new)\n",
        "\t    #    padding: string ‘same’, or ‘valid’\n",
        "\t    #    stride: tuple (sh, sw)\n",
        "\t    # Returns: parcial dev prev layer (dA_prev), kernels (dW), biases (db)\n",
        "        k_h, k_w, c_prev, c_new = self.w.shape\n",
        "        _, h_new, w_new, c_new = dZ.shape\n",
        "        m, h_x, w_x, c_prev = A_prev.shape\n",
        "        s_h, s_w = self.stride\n",
        "        x = A_prev\n",
        "        if self.padding == 'valid':\n",
        "            p_h = 0\n",
        "            p_w = 0\n",
        "\n",
        "        if self.padding == 'same':\n",
        "            p_h = np.ceil(((s_h*h_x) - s_h + k_h - h_x) / 2)\n",
        "            p_h = int(p_h)\n",
        "            p_w = np.ceil(((s_w*w_x) - s_w + k_w - w_x) / 2)\n",
        "            p_w = int(p_w)\n",
        "\n",
        "        db = np.sum(dZ, axis=(0, 1, 2), keepdims=True)\n",
        "        x_padded = np.pad(x, [(0, 0), (p_h, p_h), (p_w, p_w), (0, 0)],\n",
        "                            mode='constant', constant_values=0)\n",
        "        dW = np.zeros_like(self.w)\n",
        "        dx = np.zeros(x_padded.shape)\n",
        "        m_i = np.arange(m)\n",
        "        for i in range(m):\n",
        "            for h in range(h_new):\n",
        "                for w in range(w_new):\n",
        "                    for f in range(c_new):\n",
        "                        dx[i,\n",
        "                            h*(self.stride[0]):(h*(self.stride[0]))+k_h,\n",
        "                            w*(self.stride[1]):(w*(self.stride[1]))+k_w,\n",
        "                            :] += dZ[i, h, w, f] * self.w[:, :, :, f]\n",
        "                        dW[:, :, :, f] += x_padded[i,\n",
        "                                                h*(self.stride[0]):(h*(self.stride[0]))+k_h,\n",
        "                                                w*(self.stride[1]):(w*(self.stride[1]))+k_w,\n",
        "                                                :] * dZ[i, h, w, f]\n",
        "        if self.padding == 'same':\n",
        "            dx = dx[:, p_h:-p_h, p_w:-p_w, :]\n",
        "        else:\n",
        "            dx = dx\n",
        "        return dx, dW, db\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 372,
      "metadata": {
        "id": "JP6VwA1lyoFT"
      },
      "outputs": [],
      "source": [
        "class Pooling:\n",
        "    def __init__(self, kernel_shape,stride,mode='max'):\n",
        "        self.kernel_shape = kernel_shape\n",
        "        self.stride=stride\n",
        "        self.mode=mode\n",
        "\n",
        "    def forward_prop(self,A_prev):\n",
        "        #pool forward prop convolutional 3D image, RGB image - color\n",
        "        # Arg:\n",
        "        # A_prev: contains the output of prev layer (m, h_prev, w_prev, c_prev)\n",
        "        # W: filter for the convolution (kh, kw)\n",
        "        # stride: tuple (sh, sw)\n",
        "        # mode: indicates if max or avg\n",
        "        # Return: output of the pooling layer\n",
        "        m, h_prev, w_prev, c_prev = A_prev.shape\n",
        "        k_h = self.kernel_shape[0]\n",
        "        k_w = self.kernel_shape[1]\n",
        "\n",
        "        out_h = int(((h_prev - k_h) / (self.stride[0])) + 1)\n",
        "        out_w = int(((w_prev - k_w) / (self.stride[1])) + 1)\n",
        "        output_conv = np.zeros((m, out_h, out_w, c_prev))\n",
        "        m_A_prev = np.arange(0, m)\n",
        "        for i in range(out_h):\n",
        "            for j in range(out_w):\n",
        "                if self.mode == 'max':\n",
        "                    output_conv[m_A_prev, i, j] = np.max(\n",
        "                        A_prev[m_A_prev,\n",
        "                            i*(self.stride[0]):k_h+(i*(self.stride[0])),\n",
        "                            j*(self.stride[1]):k_w+(j*(self.stride[1]))], axis=(1, 2))\n",
        "                if self.mode == 'avg':\n",
        "                    output_conv[m_A_prev, i, j] = np.mean(\n",
        "                        A_prev[\n",
        "                            m_A_prev,\n",
        "                            i*(self.stride[0]):k_h+(i*(self.stride[0])),\n",
        "                            j*(self.stride[1]):k_w+(j*(self.stride[1]))], axis=(1, 2))\n",
        "        return output_conv\n",
        "    def backward_prop(self,dA, A_prev):\n",
        "    # back prop convolutional 3D image, RGB image - color\n",
        "    # Arg:\n",
        "    #    dA: containing the partial derivatives (m, h_new, w_new, c_new)\n",
        "    #    A_prev: contains the output of prev layer (m, h_prev, w_prev, c)\n",
        "    #    kernel.shape: filter dimensions tupple (kh, kw)\n",
        "    #    stride: tuple (sh, sw)\n",
        "    #    mode: max or avg\n",
        "    # Returns: parcial dev prev layer (dA_prev)\n",
        "        k_h = self.kernel_shape[0]\n",
        "        k_w=self.kernel_shape[1]\n",
        "        m, h_new, w_new, c_new = dA.shape\n",
        "        m, h_x, w_x, c_prev = A_prev.shape\n",
        "        s_h, s_w = self.stride\n",
        "\n",
        "        dx = np.zeros_like(A_prev)\n",
        "\n",
        "        for i in range(m):\n",
        "            for h in range(h_new):\n",
        "                for w in range(w_new):\n",
        "                    for f in range(c_new):\n",
        "                        if self.mode == 'max':\n",
        "                            tmp = A_prev[i, h*s_h:k_h+(h*s_h),\n",
        "                                        w*s_w:k_w+(w*s_w), f]\n",
        "                            mask = (tmp == np.max(tmp))\n",
        "                            dx[i,\n",
        "                            h*(s_h):(h*(s_h))+k_h,\n",
        "                            w*(s_w):(w*(s_w))+k_w,\n",
        "                            f] += dA[i, h, w, f] * mask\n",
        "                        if self.mode == 'avg':\n",
        "                            dx[i,\n",
        "                            h*(s_h):(h*(s_h))+k_h,\n",
        "                            w*(s_w):(w*(s_w))+k_w,\n",
        "                            f] += (dA[i, h, w, f])/k_h/k_w\n",
        "\n",
        "        return dx\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 373,
      "metadata": {
        "id": "rR_PjaIfyoFU"
      },
      "outputs": [],
      "source": [
        "class Reshape_layer:\n",
        "    def __init__(self):\n",
        "        self.input_shape=None\n",
        "    def forward_prop(self,X):\n",
        "        self.input_shape=X.shape\n",
        "        n_sample,_,_,_=X.shape\n",
        "        return X.reshape(-1,n_sample)\n",
        "    def backward_prop(self,X):\n",
        "        return X.reshape(self.input_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 374,
      "metadata": {
        "id": "bdV2nRCKyoFU"
      },
      "outputs": [],
      "source": [
        "class fullycolected_layer:\n",
        "    def __init__(self):\n",
        "        self.w1=None\n",
        "        self.b1=None\n",
        "        self.w2=None\n",
        "        self.b2=None\n",
        "        self.w3=None\n",
        "        self.b3=None\n",
        "    def ReLU(self,Z):\n",
        "        return np.maximum(Z, 0)\n",
        "    def softmax(self,Z):\n",
        "        A = np.exp(Z) / sum(np.exp(Z))\n",
        "        return A\n",
        "    def forward_prop(self, X,w1,b1,w2,b2,w3,b3):\n",
        "        self.w1=w1\n",
        "        self.b1=b1\n",
        "        self.w2=w2\n",
        "        self.b2=b2\n",
        "        self.w3=w3\n",
        "        self.b3=b3\n",
        "        Z1 = w1.dot(X) + b1\n",
        "        A1 = self.ReLU(Z1)\n",
        "        Z2 = w2.dot(A1) + b2\n",
        "        A2 = self.ReLU(Z2)\n",
        "        Z3 = w3.dot(A2) + b3\n",
        "        A3 = self.softmax(Z3)\n",
        "        return Z1, A1, Z2, A2, Z3, A3\n",
        "    def ReLU_deriv(self,Z):\n",
        "        return Z > 0\n",
        "    def one_hot(self,Y):\n",
        "        one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "        one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "        one_hot_Y = one_hot_Y.T\n",
        "        return one_hot_Y\n",
        "    def backward_prop(self,Z1, A1, Z2, A2, Z3, A3,X, Y):\n",
        "        m=Y.size\n",
        "        one_hot_Y = self.one_hot(Y)\n",
        "        dZ3 = A3 - one_hot_Y\n",
        "        dW3 = 1 / m * dZ3.dot(A2.T)\n",
        "        db3 = 1 / m * np.sum(dZ3)\n",
        "        dZ2 = self.w3.T.dot(dZ3) * self.ReLU_deriv(Z2)\n",
        "        dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "        db2 = 1 / m * np.sum(dZ2)\n",
        "        dZ1 = self.w2.T.dot(dZ2) * self.ReLU_deriv(Z1)\n",
        "        dW1 = 1 / m * dZ1.dot(X.T)\n",
        "        db1 = 1 / m * np.sum(dZ1)\n",
        "        d_inp=self.w1.T.dot(dZ1)\n",
        "        return dW1, db1, dW2, db2, dW3, db3, d_inp\n",
        "    def update_params(self, dW1, db1, dW2, db2, dW3, db3, alpha):\n",
        "        W1 = self.w1 - alpha * dW1\n",
        "        b1 = self.b1 - alpha * db1\n",
        "        W2 = self.w2 - alpha * dW2\n",
        "        b2 = self.b2 - alpha * db2\n",
        "        W3 = self.w3 - alpha * dW3\n",
        "        b3 = self.b3 - alpha * db3\n",
        "        return W1,b1,W2,b2,W3,b3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 375,
      "metadata": {
        "id": "XeXABP7xyoFV"
      },
      "outputs": [],
      "source": [
        "data=pd.read_csv('D:\\MachineLearning\\CNN\\Data_skincancer\\hmnist_28_28_RGB.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 376,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W35q6dfUyoFV",
        "outputId": "fcd089cf-0060-4449-ed55-6e64ac8dee5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(10015, 2353)\n"
          ]
        }
      ],
      "source": [
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 377,
      "metadata": {
        "id": "bLjSOD3RyoFW"
      },
      "outputs": [],
      "source": [
        "X=data.iloc[:,0:-1].values\n",
        "y=data.iloc[:,-1].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 378,
      "metadata": {
        "id": "_Q-NxXg0yoFW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 379,
      "metadata": {
        "id": "KG6xUx5oyoFW"
      },
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 380,
      "metadata": {
        "id": "srv-e6gmyoFW"
      },
      "outputs": [],
      "source": [
        "X_train=X_train.reshape(-1,28,28,3)\n",
        "X_test=X_test.reshape(-1,28,28,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 381,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3slclxcyoFW",
        "outputId": "f01151dc-020a-4b56-8d09-9e1a1e1a8bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8012, 28, 28, 3)\n",
            "(2003, 28, 28, 3)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 382,
      "metadata": {
        "id": "3moYMkJEyoFW"
      },
      "outputs": [],
      "source": [
        "X_train_scaled=X_train/255-0.5\n",
        "X_test_scaled=X_test/255-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 383,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3nEFUXcyoFX",
        "outputId": "53e07f93-f5d0-4c6c-832a-04c6fe4719f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(8012, 28, 28, 3)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 384,
      "metadata": {
        "id": "4tfdsT8yyoFX"
      },
      "outputs": [],
      "source": [
        "conv_layer=ConvLayer(padding='valid',activation=ReLU)\n",
        "max_pool=Pooling(kernel_shape=(3,3),stride=(3,3),mode='max')\n",
        "reshape=Reshape_layer()\n",
        "full_layer=fullycolected_layer()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 385,
      "metadata": {
        "id": "raL_gEfDyoFX"
      },
      "outputs": [],
      "source": [
        "class trainning:\n",
        "    def __init__(self,conv_layer,pool,reshape,fully):\n",
        "         self.conv_layer=conv_layer\n",
        "         self.pool=pool\n",
        "         self.full_layer=fully\n",
        "         self.reshape=reshape\n",
        "         self.w1=None\n",
        "         self.b1=None\n",
        "         self.w2=None\n",
        "         self.b2=None\n",
        "         self.w3=None\n",
        "         self.b3=None\n",
        "         self.w_conv=None\n",
        "         self.b_conv=None\n",
        "\n",
        "    def __init__conv(self,filter_size,num_filters):\n",
        "        w_conv=np.random.rand(filter_size, filter_size, 3, num_filters)\n",
        "        b_conv = np.zeros((1, 1, 1, num_filters))\n",
        "        return w_conv,b_conv\n",
        "\n",
        "    def init_fully(self,X):\n",
        "        h,w=X.shape\n",
        "        W1 = np.random.rand(7,h) - 0.5\n",
        "        b1 = np.random.rand(7, 1) - 0.5\n",
        "        W2 = np.random.rand(7, 7) - 0.5\n",
        "        b2 = np.random.rand(7, 1) - 0.5\n",
        "        W3 = np.random.rand(7, 7) - 0.5\n",
        "        b3 = np.random.rand(7, 1) - 0.5\n",
        "        return W1, b1, W2, b2, W3,b3\n",
        "    def get_predictions(self,A2):\n",
        "            return np.argmax(A2, 0)\n",
        "\n",
        "    def get_accuracy(self,predictions, Y):\n",
        "        print(predictions, Y)\n",
        "        return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "    def gradient_descent(self,X, Y, epochs=10, lr=0.01):\n",
        "        self.w_conv,self.b_conv= self.__init__conv(3,7)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            print('epochs', i)\n",
        "            print('forward')\n",
        "            conv_output = self.conv_layer.forward_prop(X,self.w_conv,self.b_conv)\n",
        "            print('conv_out',conv_output.shape)\n",
        "            pooled_output = self.pool.forward_prop(conv_output)\n",
        "            print('pool_out',pooled_output.shape)\n",
        "            flattened_output = self.reshape.forward_prop(pooled_output)\n",
        "            print('flat_out',flattened_output.shape)\n",
        "            if i==0:\n",
        "                 self.w1,self.b1,self.w2,self.b2,self.w3,self.b3=self.init_fully(flattened_output)\n",
        "            print(self.w1.max())\n",
        "            print(self.w2.max())\n",
        "            Z1, A1, Z2, A2, Z3, A3 = self.full_layer.forward_prop(flattened_output,self.w1,self.b1,self.w2,self.b2,self.w3,self.b3)\n",
        "            print('backward')\n",
        "            dw1, db1, dw2, db2, dw3,db3,dZ1 = self.full_layer.backward_prop(Z1, A1, Z2, A2, Z3, A3, flattened_output, Y)\n",
        "            dZ1_reshape=self.reshape.backward_prop(dZ1)\n",
        "            print('dZ1_reshape',dZ1_reshape.shape)\n",
        "            dl_dmax_pool=self.pool.backward_prop(dZ1_reshape,conv_output)\n",
        "            print('dmax_pool',dl_dmax_pool.shape)\n",
        "            dZ=ReLU_deriv(dl_dmax_pool)\n",
        "            print('dz',dZ.shape)\n",
        "            _,dw_conv,db_conv=conv_layer.backward_prop(dZ,X_train_scaled)\n",
        "            print('dw_conv',dw_conv.shape)\n",
        "            print('db_conv',db_conv.shape)\n",
        "            self.w1 = self.w1 - lr * dw1\n",
        "            self.b1 = self.b1 - lr * db1\n",
        "            self.w2 = self.w2 - lr * dw2\n",
        "            self.b2 = self.b2 - lr * db2\n",
        "            self.w3 = self.w3 - lr * dw3\n",
        "            self.b3 = self.b3 - lr * db3\n",
        "            self.w_conv=self.w_conv-lr*dw_conv\n",
        "            self.b_conv=self.b_conv-lr*db_conv\n",
        "            predictions = self.get_predictions(A3)\n",
        "            print(self.get_accuracy(predictions, Y))\n",
        "        return self.w1, self.b1, self.w2, self.b2,self.w3,self.b3,self.w_conv,self.b_conv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 386,
      "metadata": {
        "id": "1T-9cTusHHMA"
      },
      "outputs": [],
      "source": [
        "conv_layer=ConvLayer(padding='valid',activation=ReLU)\n",
        "max_pool=Pooling(kernel_shape=(2,2),stride=(2,2),mode='max')\n",
        "reshape=Reshape_layer()\n",
        "full_layer=fullycolected_layer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 387,
      "metadata": {
        "id": "PBGxbp2WyoFX"
      },
      "outputs": [],
      "source": [
        "trainning_cnn=trainning(conv_layer,max_pool,reshape,full_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rFNULZhHKf5",
        "outputId": "fdb33a0b-ded5-4520-e060-ea3874195fa6"
      },
      "outputs": [],
      "source": [
        "W1, b1, W2, b2,W3,b3,w_conv=trainning_cnn.gradient_descent(X_train_scaled,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKcQrwcLtitX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
